# feature-selection


 LDA
 QDA
 Linear regression of an indicator metrix (not LDA !)
 Maximum likelihood method (Iterated Weighted Least SquaresIWLS: linear models fitting using Weighted Least Squares)
 Blyth estimator
 Regularised (Lasso) estimator
 Ridge estimator
 Naive Bayes method

 Regularised LDA (RLDA)
 LDA with diagonal covariance matrix (LDA + naive Bayes)
 Nearest Shrunken Centroids (NSC)
 Logistic ridge regression
 Lasso
 Two-step procedures: variable selection + ’classic’ classification
 Random Subspace Method

 Bootstrap
 Bagging (Bootstrap Aggregating)
 Algorithm AdaBoost.M1 (AdaptiveBoosting)
 Alternatives to AdaBoost
 Functional gradient algorithm FAG aka gradient boosting
 XGBoost
 Boosted stump (AdaBoost)2
 Random subspace method
 Random Forests

Random Forests allow for simple and intuitive ranking ofvariables according to theirimportance. 
Variable Importance (VI) index is e.g. average of thedifferences calculated for all trees of the forest.Other methods which use sample splitting/resamplingtechniques for selection:•Stability lasso•Multi-split (lecture: analysis of high-dimensional data

 Boruta algorithm , Kursa, Rudnicki (2010)
 Monte Carlo Feature Selection MCFS
 Extremely Randomized Trees

 Rosenblatt perceptron 
 SVM – Support Vector Machines
 Approach based on loss functions and their correspondingrisks
 Kernels
 Kernel estimator 
 Cross-validation smoothing parameter
 Nearest neighbor method
 Learning Vector Quantization

 Variability measures for nominal variables: entropy, Gini index
 Nominal attributes 
 Regression trees
 Pruning
 1SE rule

 PCA
 Partial Least Squares Regression, PLSR

 MARS Multivriate Addaptive Regression Splines
 GCV
 Projection pursuit regression (PPR) method
 